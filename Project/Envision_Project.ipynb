{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T15:19:39.724975Z",
     "start_time": "2019-08-21T15:19:23.199Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from itertools import permutations\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer, sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T15:18:14.448047Z",
     "start_time": "2019-08-21T15:18:08.765Z"
    }
   },
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    '''Recursively Flatten a nested List'''\n",
    "    return sum( ([x] if not isinstance(x, list) else flatten(x) for x in lst), [] )\n",
    "\n",
    "def pre_process(data):\n",
    "    '''Returns cleaned DataFrame'''\n",
    "    \n",
    "    # load stop words and lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # characters to replace in data\n",
    "    to_replace_with_space = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~1234567890' \n",
    "    to_replace_with_empty_string = '-'\n",
    "\n",
    "    # sentence tokenization and split at br tags\n",
    "    data['Data'] = data['Data'].apply(lambda text: flatten([sentence.split('<br />') for sentence in sent_tokenize(text)]))\n",
    "\n",
    "    # convert to lowercase, replace punctuations and strip\n",
    "    data['Data'] = data['Data'].apply(lambda strlist: list(map(lambda x: x.lower().translate(str.maketrans(to_replace_with_space, ''.join([' ' for _ in to_replace_with_space]))).translate(str.maketrans(({char: None for char in to_replace_with_empty_string}))).strip(), strlist)))\n",
    "\n",
    "    # word tokenize, remove small words and lemmatize\n",
    "    data['Data'] = data['Data'].apply(lambda strlist: list(map(lambda l: [lemmatizer.lemmatize(w) for w in l if len(w) > 2], [word_tokenize(sent) for sent in strlist])))\n",
    "\n",
    "    # remove empty lists\n",
    "    data['Data'] = data['Data'].apply(lambda ll: list(filter(lambda l: l, ll)))\n",
    "    \n",
    "    # Label Encode Critical_Findings column\n",
    "    data['Critical_Finding'] = data['Critical_Finding'].replace(['None', 'Complete Critical Finding', 'Complete Physician Decline'], [0, 1, 1])\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T15:41:30.691935Z",
     "start_time": "2019-08-20T15:41:30.432973Z"
    }
   },
   "outputs": [],
   "source": [
    "data_critical = pd.read_csv('./critical-findings-sample-data-20180601-20180901.csv')\n",
    "data_non_critical = pd.read_csv('./non-critical-findings-sample-data-20180601-20180901.csv')\n",
    "\n",
    "data = pd.concat([data_critical, data_non_critical]).reset_index(drop=True)\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T15:41:34.463407Z",
     "start_time": "2019-08-20T15:41:30.778371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modality</th>\n",
       "      <th>Critical_Finding</th>\n",
       "      <th>Category</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CR</td>\n",
       "      <td>1</td>\n",
       "      <td>Other</td>\n",
       "      <td>[[chest, view], [indication, picc, line, place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT</td>\n",
       "      <td>1</td>\n",
       "      <td>Acute Vascular Event</td>\n",
       "      <td>[[study, brain, without, contrast], [reason, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[study, renal, ultrasound, complete], [reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CTA</td>\n",
       "      <td>1</td>\n",
       "      <td>Acute Vascular Event</td>\n",
       "      <td>[[examination, cta, head, with, contrast], [in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CR</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[study, xray, chest], [reason, for, exam, fem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Modality  Critical_Finding              Category  \\\n",
       "0       CR                 1                 Other   \n",
       "1       CT                 1  Acute Vascular Event   \n",
       "2       US                 0                   NaN   \n",
       "3      CTA                 1  Acute Vascular Event   \n",
       "4       CR                 0                   NaN   \n",
       "\n",
       "                                                Data  \n",
       "0  [[chest, view], [indication, picc, line, place...  \n",
       "1  [[study, brain, without, contrast], [reason, f...  \n",
       "2  [[study, renal, ultrasound, complete], [reason...  \n",
       "3  [[examination, cta, head, with, contrast], [in...  \n",
       "4  [[study, xray, chest], [reason, for, exam, fem...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pre_process(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Embedding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T17:32:44.898823Z",
     "start_time": "2019-08-21T17:32:44.888855Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_glove(cleaned_data):\n",
    "    '''Converts cleaned data to GloVe Embeddings'''\n",
    "    glove_embeddings = pickle.load(open('GloVe_Embeddings_Dict.pickle', 'rb'))\n",
    "    glove_data = cleaned_data.copy()\n",
    "\n",
    "    glove_data['Data'] = glove_data['Data'].apply(lambda ll: sum(list(map(lambda word: glove_embeddings[word] if word in glove_embeddings else np.zeros(300), flatten(ll)))))\n",
    "    dim_columns = []\n",
    "    for i in range(300):\n",
    "        glove_data['dim_' + str(i)] = glove_data.Data.apply(lambda x: x[i])\n",
    "        dim_columns.append('dim_' + str(i))\n",
    "    return glove_data[dim_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T17:31:42.773045Z",
     "start_time": "2019-08-21T17:31:41.669234Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_pubmed(cleaned_data):\n",
    "    '''Converts cleaned data to PubMed Embeddings'''\n",
    "    pubmed_embeddings = pickle.load(open('PubMed_Embeddings_Dict.pickle', 'rb'))\n",
    "    pubmed_data = cleaned_data.copy()\n",
    "\n",
    "    pubmed_data['Data'] = pubmed_data['Data'].apply(lambda ll: sum(list(map(lambda word: pubmed_embeddings[word] if word in pubmed_embeddings else np.zeros(200), flatten(ll)))))\n",
    "    dim_columns = []\n",
    "    for i in range(200):\n",
    "        pubmed_data['dim_' + str(i)] = pubmed_data.Data.apply(lambda x: x[i])\n",
    "        dim_columns.append('dim_' + str(i))\n",
    "    return pubmed_data[dim_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:19:03.806020Z",
     "start_time": "2019-08-21T19:19:03.134280Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_bioasq(cleaned_data):\n",
    "    '''Converts cleaned data to BioASQ Embeddings'''\n",
    "    bioasq_embeddings = pickle.load(open('BioASQ_Embeddings_Dict.pickle', 'rb'))\n",
    "    bioasq_data = cleaned_data.copy()\n",
    "\n",
    "    bioasq_data['Data'] = bioasq_data['Data'].apply(lambda ll: sum(list(map(lambda word: bioasq_embeddings[word] if word in bioasq_embeddings else np.zeros(200), flatten(ll)))))\n",
    "    dim_columns = []\n",
    "    for i in range(200):\n",
    "        bioasq_data['dim_' + str(i)] = bioasq_data.Data.apply(lambda x: x[i])\n",
    "        dim_columns.append('dim_' + str(i))\n",
    "    return bioasq_data[dim_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T15:32:35.768996Z",
     "start_time": "2019-08-21T15:32:35.761021Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_tfidf(cleaned_data, gram):\n",
    "    '''Converts cleaned data to TF-IDF vectors'''\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,gram))\n",
    "\n",
    "    tfidf = tfidf_vectorizer.fit_transform(cleaned_data['Data'].apply(lambda x: ' '.join(flatten(x))))\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T15:30:49.413572Z",
     "start_time": "2019-08-20T15:30:49.408548Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_bow(cleaned_data):\n",
    "    '''Converts cleaned data to BOW vectors'''\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    \n",
    "    bow = count_vectorizer.fit_transform(cleaned_data['Data'].apply(lambda x: ' '.join(flatten(x)))) \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize embeddings dictionary and models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:25:30.217389Z",
     "start_time": "2019-08-21T19:20:05.379322Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = {\n",
    "    \"BOW\": to_bow(data),\n",
    "    \"TFIDF-1Gram\": to_tfidf(data, 1),\n",
    "    \"TFIDF-2Gram\": to_tfidf(data, 2),\n",
    "    \"TFIDF-3Gram\": to_tfidf(data, 3),\n",
    "    \"GloVe\": to_glove(data),\n",
    "    \"PubMed\": to_pubmed(data),\n",
    "    \"BioASQ\": to_bioasq(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T06:32:06.869820Z",
     "start_time": "2019-08-21T06:32:06.859852Z"
    }
   },
   "outputs": [],
   "source": [
    "sklearn_models = {\n",
    "    \"LogisticRegression\": LogisticRegression(solver='lbfgs', max_iter=1000),\n",
    "    \"MLPClassifier\": MLPClassifier(solver='adam', alpha=1e-4, hidden_layer_sizes=(1000, 300, 10), random_state=1),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"ExtraTreeClassifier\": ExtraTreeClassifier(),\n",
    "    \"SVC\": SVC(kernel='linear'),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(n_neighbors=2),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    \"BaggingClassifier\": BaggingClassifier(),\n",
    "    \"ExtraTreesClassifier\": ExtraTreesClassifier(n_estimators=100),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(loss=\"deviance\", n_estimators=100),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic function to run Sklearn Models with Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T15:34:11.719599Z",
     "start_time": "2019-08-21T15:34:10.708625Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_sklearn(model, embedding, pickle_model=False, use_pickle=False):\n",
    "    '''Run Sklearn Model with Embedding. Returns Accuracy'''\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(embeddings[embedding], data['Critical_Finding'], test_size = 0.2, random_state=4)\n",
    "    if use_pickle and os.path.exists(model + '_' + embedding + '.pickle'):\n",
    "        trained_model = pickle.load(open(model + '_' + embedding + '.pickle', 'rb'))\n",
    "        y_pred = trained_model.predict(X_test)\n",
    "    else:\n",
    "        sklearn_models[model].fit(X_train, y_train)\n",
    "        if pickle_model:\n",
    "            pickle.dump(sklearn_models[model], open(model + '_' + embedding + '.pickle', 'wb'))\n",
    "\n",
    "        y_pred = sklearn_models[model].predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T12:54:16.092763Z",
     "start_time": "2019-08-21T12:54:16.084786Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_dense_model(train_data_shape):\n",
    "    '''Generate Standard Keras DenseNet'''\n",
    "    # create model\n",
    "    dense_model = Sequential()\n",
    "\n",
    "    #add model layers\n",
    "    dense_model.add(Dense(1000, activation='relu', input_shape=(train_data_shape[2],)))\n",
    "    dense_model.add(Dense(600, activation='relu'))\n",
    "    dense_model.add(Dense(200, activation='relu'))\n",
    "    dense_model.add(Dense(30, activation='relu'))\n",
    "    dense_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile model\n",
    "    dense_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T13:07:00.308792Z",
     "start_time": "2019-08-21T13:07:00.303769Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_lstm_model(train_data_shape):\n",
    "    '''Generate an LSTM model'''\n",
    "    # create model\n",
    "    lstm_model = Sequential()\n",
    "\n",
    "    #add model layers\n",
    "    lstm_model.add(LSTM(100, input_shape=(None, train_data_shape[2])))\n",
    "    lstm_model.add(Dense(80, activation='relu'))\n",
    "    lstm_model.add(Dense(40, activation='relu'))\n",
    "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile model\n",
    "    lstm_model.compile(optimizer='adam', loss='poisson', metrics=['accuracy'])\n",
    "    \n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T14:45:10.530174Z",
     "start_time": "2019-08-21T14:45:10.522231Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_bilstm_model(train_data_shape):\n",
    "    '''Generate a Bi-directional LSTM'''\n",
    "    # create model\n",
    "    bilstm_model = Sequential()\n",
    "\n",
    "    #add model layers\n",
    "    bilstm_model.add(Bidirectional(LSTM(100, input_shape=(None, train_data_shape[2]))))\n",
    "    bilstm_model.add(Dense(80, activation='relu'))\n",
    "    bilstm_model.add(Dense(40, activation='relu'))\n",
    "    bilstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile model\n",
    "    bilstm_model.compile(optimizer='adam', loss='poisson', metrics=['accuracy'])\n",
    "    \n",
    "    return bilstm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Keras Models Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T14:47:34.129482Z",
     "start_time": "2019-08-21T14:47:33.984470Z"
    }
   },
   "outputs": [],
   "source": [
    "keras_models = {\n",
    "    \"DenseNet\": lambda shape: generate_dense_model(shape),\n",
    "    \"LSTM\": lambda shape: generate_lstm_model(shape),\n",
    "    \"BiLSTM\": lambda shape: generate_bilstm_model(shape),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T14:54:17.216863Z",
     "start_time": "2019-08-21T14:54:17.202899Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_for_keras(embedding):\n",
    "    '''Get Training and Test Data for Keras Models with provided embedding'''\n",
    "    \n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(embeddings[embedding], data['Critical_Finding'], test_size = 0.3, random_state=42)\n",
    "    \n",
    "    # Reshape for Keras Models \n",
    "    if str(type(X_train))[8:13] == 'scipy':\n",
    "        X_train = X_train.toarray().reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = X_test.toarray().reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    else:\n",
    "        X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    return X_train, X_test, y_train.values, y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T15:06:39.546746Z",
     "start_time": "2019-08-21T15:06:39.532752Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_keras(model, embedding, pickle_model=False, use_pickle=False):\n",
    "    '''Run Keras Model with Embedding. Returns Accuracy'''\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = get_data_for_keras(embedding)\n",
    "    \n",
    "    if use_pickle and os.path.exists(model + '_' + embedding + '.pickle'):\n",
    "        trained_model = pickle.load(open(model + '_' + embedding + '.pickle', 'rb'))\n",
    "        if model == \"DenseNet\":\n",
    "            accuracy = trained_model.evaluate(X_test.reshape(X_test.shape[0], X_test.shape[2]), y_test)[1]*100\n",
    "        else:\n",
    "            accuracy = trained_model.evaluate(X_test, y_test)[1]*100\n",
    "    else:\n",
    "        if model == \"DenseNet\":\n",
    "            model_obj = keras_models[model](X_train.shape)\n",
    "            model_obj.fit(X_train.reshape(X_train.shape[0], X_train.shape[2]), y_train, validation_split=0.3, epochs=300, batch_size=1000, verbose=0)\n",
    "\n",
    "            accuracy = model_obj.evaluate(X_test.reshape(X_test.shape[0], X_test.shape[2]), y_test)[1]*100\n",
    "        else:\n",
    "            model_obj = keras_models[model](X_train.shape)\n",
    "            model_obj.fit(X_train, y_train, validation_split=0.3, epochs=300, batch_size=1000, verbose=0)\n",
    "\n",
    "            accuracy = model_obj.evaluate(X_test, y_test)[1]*100\n",
    "        if pickle_model:\n",
    "            pickle.dump(model_obj, open(model + '_' + embedding + '.pickle', 'wb'))\n",
    "            \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T18:07:57.913671Z",
     "start_time": "2019-08-21T18:07:57.903699Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_all(skip_old_models=False):\n",
    "    '''Run all combinations of sklearn and Keras models. Returns a Pandas DataFrame containing the results'''\n",
    "    \n",
    "    sklearn_run_combinations = sorted([(model, embedding) for model in sklearn_models.keys() for embedding in embeddings.keys()], key=lambda x: x[1])\n",
    "    keras_run_combinations = sorted([(model, embedding) for model in keras_models.keys() for embedding in embeddings.keys()], key=lambda x: x[1])\n",
    "    \n",
    "    results_df = pd.DataFrame(columns=['Model', 'Embedding', 'Accuracy'])\n",
    "    start_from = 0\n",
    "    \n",
    "    for i, r in enumerate(sklearn_run_combinations):\n",
    "        print('Model: {}\\tEmbedding: {}'.format(r[0], r[1]))\n",
    "        if skip_old_models and os.path.exists(r[0] + '_' + r[1] + '.pickle'):\n",
    "            print('Skipping.....\\n')\n",
    "            continue\n",
    "        results_df.loc[i] = [r[0], r[1], run_sklearn(r[0], r[1], pickle_model=True, use_pickle=True)]\n",
    "        start_from = i\n",
    "    \n",
    "    for i, r in enumerate(keras_run_combinations, start=start_from + 1):\n",
    "        print('Model: {}\\tEmbedding: {}'.format(r[0], r[1]))\n",
    "        if skip_old_models and os.path.exists(r[0] + '_' + r[1] + '.pickle'):\n",
    "            print('Skipping.....\\n')\n",
    "            continue\n",
    "        results_df.loc[i] = [r[0], r[1], run_keras(r[0], r[1], pickle_model=True, use_pickle=True)]\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T20:15:32.801913Z",
     "start_time": "2019-08-21T19:55:10.568316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\tEmbedding: BOW\n",
      "Model: MLPClassifier\tEmbedding: BOW\n",
      "Model: DecisionTreeClassifier\tEmbedding: BOW\n",
      "Model: ExtraTreeClassifier\tEmbedding: BOW\n",
      "Model: SVC\tEmbedding: BOW\n",
      "Model: KNeighborsClassifier\tEmbedding: BOW\n",
      "Model: AdaBoostClassifier\tEmbedding: BOW\n",
      "Model: BaggingClassifier\tEmbedding: BOW\n",
      "Model: ExtraTreesClassifier\tEmbedding: BOW\n",
      "Model: GradientBoostingClassifier\tEmbedding: BOW\n",
      "Model: RandomForestClassifier\tEmbedding: BOW\n",
      "Model: LogisticRegression\tEmbedding: BioASQ\n",
      "Model: MLPClassifier\tEmbedding: BioASQ\n",
      "Model: DecisionTreeClassifier\tEmbedding: BioASQ\n",
      "Model: ExtraTreeClassifier\tEmbedding: BioASQ\n",
      "Model: SVC\tEmbedding: BioASQ\n",
      "Model: KNeighborsClassifier\tEmbedding: BioASQ\n",
      "Model: AdaBoostClassifier\tEmbedding: BioASQ\n",
      "Model: BaggingClassifier\tEmbedding: BioASQ\n",
      "Model: ExtraTreesClassifier\tEmbedding: BioASQ\n",
      "Model: GradientBoostingClassifier\tEmbedding: BioASQ\n",
      "Model: RandomForestClassifier\tEmbedding: BioASQ\n",
      "Model: LogisticRegression\tEmbedding: GloVe\n",
      "Model: MLPClassifier\tEmbedding: GloVe\n",
      "Model: DecisionTreeClassifier\tEmbedding: GloVe\n",
      "Model: ExtraTreeClassifier\tEmbedding: GloVe\n",
      "Model: SVC\tEmbedding: GloVe\n",
      "Model: KNeighborsClassifier\tEmbedding: GloVe\n",
      "Model: AdaBoostClassifier\tEmbedding: GloVe\n",
      "Model: BaggingClassifier\tEmbedding: GloVe\n",
      "Model: ExtraTreesClassifier\tEmbedding: GloVe\n",
      "Model: GradientBoostingClassifier\tEmbedding: GloVe\n",
      "Model: RandomForestClassifier\tEmbedding: GloVe\n",
      "Model: LogisticRegression\tEmbedding: PubMed\n",
      "Model: MLPClassifier\tEmbedding: PubMed\n",
      "Model: DecisionTreeClassifier\tEmbedding: PubMed\n",
      "Model: ExtraTreeClassifier\tEmbedding: PubMed\n",
      "Model: SVC\tEmbedding: PubMed\n",
      "Model: KNeighborsClassifier\tEmbedding: PubMed\n",
      "Model: AdaBoostClassifier\tEmbedding: PubMed\n",
      "Model: BaggingClassifier\tEmbedding: PubMed\n",
      "Model: ExtraTreesClassifier\tEmbedding: PubMed\n",
      "Model: GradientBoostingClassifier\tEmbedding: PubMed\n",
      "Model: RandomForestClassifier\tEmbedding: PubMed\n",
      "Model: LogisticRegression\tEmbedding: TFIDF-1Gram\n",
      "Model: MLPClassifier\tEmbedding: TFIDF-1Gram\n",
      "Model: DecisionTreeClassifier\tEmbedding: TFIDF-1Gram\n",
      "Model: ExtraTreeClassifier\tEmbedding: TFIDF-1Gram\n",
      "Model: SVC\tEmbedding: TFIDF-1Gram\n",
      "Model: KNeighborsClassifier\tEmbedding: TFIDF-1Gram\n",
      "Model: AdaBoostClassifier\tEmbedding: TFIDF-1Gram\n",
      "Model: BaggingClassifier\tEmbedding: TFIDF-1Gram\n",
      "Model: ExtraTreesClassifier\tEmbedding: TFIDF-1Gram\n",
      "Model: GradientBoostingClassifier\tEmbedding: TFIDF-1Gram\n",
      "Model: RandomForestClassifier\tEmbedding: TFIDF-1Gram\n",
      "Model: LogisticRegression\tEmbedding: TFIDF-2Gram\n",
      "Model: MLPClassifier\tEmbedding: TFIDF-2Gram\n",
      "Model: DecisionTreeClassifier\tEmbedding: TFIDF-2Gram\n",
      "Model: ExtraTreeClassifier\tEmbedding: TFIDF-2Gram\n",
      "Model: SVC\tEmbedding: TFIDF-2Gram\n",
      "Model: KNeighborsClassifier\tEmbedding: TFIDF-2Gram\n",
      "Model: AdaBoostClassifier\tEmbedding: TFIDF-2Gram\n",
      "Model: BaggingClassifier\tEmbedding: TFIDF-2Gram\n",
      "Model: ExtraTreesClassifier\tEmbedding: TFIDF-2Gram\n",
      "Model: GradientBoostingClassifier\tEmbedding: TFIDF-2Gram\n",
      "Model: RandomForestClassifier\tEmbedding: TFIDF-2Gram\n",
      "Model: LogisticRegression\tEmbedding: TFIDF-3Gram\n",
      "Model: MLPClassifier\tEmbedding: TFIDF-3Gram\n",
      "Model: DecisionTreeClassifier\tEmbedding: TFIDF-3Gram\n",
      "Model: ExtraTreeClassifier\tEmbedding: TFIDF-3Gram\n",
      "Model: SVC\tEmbedding: TFIDF-3Gram\n",
      "Model: KNeighborsClassifier\tEmbedding: TFIDF-3Gram\n",
      "Model: AdaBoostClassifier\tEmbedding: TFIDF-3Gram\n",
      "Model: BaggingClassifier\tEmbedding: TFIDF-3Gram\n",
      "Model: ExtraTreesClassifier\tEmbedding: TFIDF-3Gram\n",
      "Model: GradientBoostingClassifier\tEmbedding: TFIDF-3Gram\n",
      "Model: RandomForestClassifier\tEmbedding: TFIDF-3Gram\n",
      "Model: DenseNet\tEmbedding: BOW\n",
      "300/300 [==============================] - 5s 15ms/step\n",
      "Model: LSTM\tEmbedding: BOW\n",
      "300/300 [==============================] - 5s 16ms/step\n",
      "Model: BiLSTM\tEmbedding: BOW\n",
      "300/300 [==============================] - 5s 16ms/step\n",
      "Model: DenseNet\tEmbedding: BioASQ\n",
      "300/300 [==============================] - 6s 19ms/step\n",
      "Model: LSTM\tEmbedding: BioASQ\n",
      "300/300 [==============================] - 5s 15ms/step\n",
      "Model: BiLSTM\tEmbedding: BioASQ\n",
      "300/300 [==============================] - 4s 14ms/step\n",
      "Model: DenseNet\tEmbedding: GloVe\n",
      "300/300 [==============================] - 4s 14ms/step\n",
      "Model: LSTM\tEmbedding: GloVe\n",
      "300/300 [==============================] - 4s 14ms/step\n",
      "Model: BiLSTM\tEmbedding: GloVe\n",
      "300/300 [==============================] - 5s 17ms/step\n",
      "Model: DenseNet\tEmbedding: PubMed\n",
      "300/300 [==============================] - 5s 16ms/step\n",
      "Model: LSTM\tEmbedding: PubMed\n",
      "300/300 [==============================] - 4s 14ms/step\n",
      "Model: BiLSTM\tEmbedding: PubMed\n",
      "300/300 [==============================] - 6s 20ms/step\n",
      "Model: DenseNet\tEmbedding: TFIDF-1Gram\n",
      "300/300 [==============================] - 6s 19ms/step\n",
      "Model: LSTM\tEmbedding: TFIDF-1Gram\n",
      "300/300 [==============================] - 6s 19ms/step\n",
      "Model: BiLSTM\tEmbedding: TFIDF-1Gram\n",
      "300/300 [==============================] - 6s 19ms/step\n",
      "Model: DenseNet\tEmbedding: TFIDF-2Gram\n",
      "300/300 [==============================] - 6s 20ms/step\n",
      "Model: LSTM\tEmbedding: TFIDF-2Gram\n",
      "300/300 [==============================] - 6s 21ms/step\n",
      "Model: BiLSTM\tEmbedding: TFIDF-2Gram\n",
      "300/300 [==============================] - 8s 25ms/step\n",
      "Model: DenseNet\tEmbedding: TFIDF-3Gram\n",
      "300/300 [==============================] - 7s 24ms/step\n",
      "Model: LSTM\tEmbedding: TFIDF-3Gram\n",
      "300/300 [==============================] - 8s 26ms/step\n",
      "Model: BiLSTM\tEmbedding: TFIDF-3Gram\n",
      "300/300 [==============================] - 10s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "results_df = run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T20:15:34.255215Z",
     "start_time": "2019-08-21T20:15:33.898487Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(results_df, open('results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T20:30:33.277046Z",
     "start_time": "2019-08-21T20:30:33.231454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Model    Embedding   Accuracy\n",
      "0           LogisticRegression          BOW  95.000000\n",
      "1                MLPClassifier          BOW  96.000000\n",
      "2       DecisionTreeClassifier          BOW  92.000000\n",
      "3          ExtraTreeClassifier          BOW  90.500000\n",
      "4                          SVC          BOW  94.000000\n",
      "5         KNeighborsClassifier          BOW  86.500000\n",
      "6           AdaBoostClassifier          BOW  87.000000\n",
      "7            BaggingClassifier          BOW  92.500000\n",
      "8         ExtraTreesClassifier          BOW  97.000000\n",
      "9   GradientBoostingClassifier          BOW  92.000000\n",
      "10      RandomForestClassifier          BOW  97.000000\n",
      "11          LogisticRegression       BioASQ  93.000000\n",
      "12               MLPClassifier       BioASQ  90.500000\n",
      "13      DecisionTreeClassifier       BioASQ  87.000000\n",
      "14         ExtraTreeClassifier       BioASQ  91.000000\n",
      "15                         SVC       BioASQ  93.000000\n",
      "16        KNeighborsClassifier       BioASQ  88.000000\n",
      "17          AdaBoostClassifier       BioASQ  91.000000\n",
      "18           BaggingClassifier       BioASQ  93.000000\n",
      "19        ExtraTreesClassifier       BioASQ  94.000000\n",
      "20  GradientBoostingClassifier       BioASQ  93.500000\n",
      "21      RandomForestClassifier       BioASQ  94.500000\n",
      "22          LogisticRegression        GloVe  91.500000\n",
      "23               MLPClassifier        GloVe  92.000000\n",
      "24      DecisionTreeClassifier        GloVe  91.500000\n",
      "25         ExtraTreeClassifier        GloVe  91.500000\n",
      "26                         SVC        GloVe  89.500000\n",
      "27        KNeighborsClassifier        GloVe  87.500000\n",
      "28          AdaBoostClassifier        GloVe  90.500000\n",
      "29           BaggingClassifier        GloVe  93.000000\n",
      "30        ExtraTreesClassifier        GloVe  96.000000\n",
      "31  GradientBoostingClassifier        GloVe  93.500000\n",
      "32      RandomForestClassifier        GloVe  94.500000\n",
      "33          LogisticRegression       PubMed  94.500000\n",
      "34               MLPClassifier       PubMed  94.000000\n",
      "35      DecisionTreeClassifier       PubMed  90.500000\n",
      "36         ExtraTreeClassifier       PubMed  92.500000\n",
      "37                         SVC       PubMed  92.500000\n",
      "38        KNeighborsClassifier       PubMed  89.000000\n",
      "39          AdaBoostClassifier       PubMed  90.500000\n",
      "40           BaggingClassifier       PubMed  92.000000\n",
      "41        ExtraTreesClassifier       PubMed  95.000000\n",
      "42  GradientBoostingClassifier       PubMed  93.000000\n",
      "43      RandomForestClassifier       PubMed  95.000000\n",
      "44          LogisticRegression  TFIDF-1Gram  91.500000\n",
      "45               MLPClassifier  TFIDF-1Gram  95.500000\n",
      "46      DecisionTreeClassifier  TFIDF-1Gram  91.500000\n",
      "47         ExtraTreeClassifier  TFIDF-1Gram  91.000000\n",
      "48                         SVC  TFIDF-1Gram  93.500000\n",
      "49        KNeighborsClassifier  TFIDF-1Gram  89.500000\n",
      "50          AdaBoostClassifier  TFIDF-1Gram  91.000000\n",
      "51           BaggingClassifier  TFIDF-1Gram  93.000000\n",
      "52        ExtraTreesClassifier  TFIDF-1Gram  97.000000\n",
      "53  GradientBoostingClassifier  TFIDF-1Gram  95.000000\n",
      "54      RandomForestClassifier  TFIDF-1Gram  95.000000\n",
      "55          LogisticRegression  TFIDF-2Gram  94.500000\n",
      "56               MLPClassifier  TFIDF-2Gram  96.500000\n",
      "57      DecisionTreeClassifier  TFIDF-2Gram  90.000000\n",
      "58         ExtraTreeClassifier  TFIDF-2Gram  93.000000\n",
      "59                         SVC  TFIDF-2Gram  95.500000\n",
      "60        KNeighborsClassifier  TFIDF-2Gram  88.000000\n",
      "61          AdaBoostClassifier  TFIDF-2Gram  88.500000\n",
      "62           BaggingClassifier  TFIDF-2Gram  92.000000\n",
      "63        ExtraTreesClassifier  TFIDF-2Gram  97.000000\n",
      "64  GradientBoostingClassifier  TFIDF-2Gram  95.500000\n",
      "65      RandomForestClassifier  TFIDF-2Gram  95.000000\n",
      "66          LogisticRegression  TFIDF-3Gram  94.500000\n",
      "67               MLPClassifier  TFIDF-3Gram  96.000000\n",
      "68      DecisionTreeClassifier  TFIDF-3Gram  93.500000\n",
      "69         ExtraTreeClassifier  TFIDF-3Gram  88.000000\n",
      "70                         SVC  TFIDF-3Gram  96.000000\n",
      "71        KNeighborsClassifier  TFIDF-3Gram  88.500000\n",
      "72          AdaBoostClassifier  TFIDF-3Gram  92.500000\n",
      "73           BaggingClassifier  TFIDF-3Gram  94.500000\n",
      "74        ExtraTreesClassifier  TFIDF-3Gram  96.000000\n",
      "75  GradientBoostingClassifier  TFIDF-3Gram  94.500000\n",
      "76      RandomForestClassifier  TFIDF-3Gram  93.500000\n",
      "77                    DenseNet          BOW  92.000000\n",
      "78                        LSTM          BOW  92.000000\n",
      "79                      BiLSTM          BOW  92.000000\n",
      "80                    DenseNet       BioASQ  89.000000\n",
      "81                        LSTM       BioASQ  91.666667\n",
      "82                      BiLSTM       BioASQ  92.000000\n",
      "83                    DenseNet        GloVe  90.666667\n",
      "84                        LSTM        GloVe  92.333333\n",
      "85                      BiLSTM        GloVe  92.333333\n",
      "86                    DenseNet       PubMed  92.000000\n",
      "87                        LSTM       PubMed  90.666667\n",
      "88                      BiLSTM       PubMed  92.000000\n",
      "89                    DenseNet  TFIDF-1Gram  91.666667\n",
      "90                        LSTM  TFIDF-1Gram  91.000000\n",
      "91                      BiLSTM  TFIDF-1Gram  91.333333\n",
      "92                    DenseNet  TFIDF-2Gram  92.666667\n",
      "93                        LSTM  TFIDF-2Gram  91.666667\n",
      "94                      BiLSTM  TFIDF-2Gram  92.000000\n",
      "95                    DenseNet  TFIDF-3Gram  93.333333\n",
      "96                        LSTM  TFIDF-3Gram  92.000000\n",
      "97                      BiLSTM  TFIDF-3Gram  91.666667\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
